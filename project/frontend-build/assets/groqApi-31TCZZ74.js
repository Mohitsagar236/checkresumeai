var F=Object.defineProperty;var X=(r,e,t)=>e in r?F(r,e,{enumerable:!0,configurable:!0,writable:!0,value:t}):r[e]=t;var R=(r,e,t)=>X(r,typeof e!="symbol"?e+"":e,t);import{m as C}from"./heavy-pages-C8trrwvk.js";import"./react-vendor-GyENwBWe.js";import"./vendor-x74MHN4I.js";import"./api-auth-dsn4brwG.js";import"./utilities-CaqBGVfY.js";import"./pdfjs-Cev1TANf.js";import"./ui-components-C10pzA45.js";class J{constructor(e){R(this,"cache",new Map);R(this,"config");this.config={defaultTTL:60*60*1e3,maxEntries:50,enableInDev:!0,debug:!1,...e},this.logDebug("API Cache initialized",this.config)}async get(e,t,o){const{ttl:s=this.config.defaultTTL,skipCache:a=!1}=o||{};if(!this.isCacheEnabled()||a){const S=await t();return this.logDebug(`Cache bypassed for key: ${e}`),S}const n=this.cache.get(e),c=Date.now();if(n&&n.expiresAt>c)return this.logDebug(`Cache HIT for key: ${e}`),n.data;this.logDebug(`Cache MISS for key: ${e}`);const u=await t();return this.set(e,u,s),u}set(e,t,o=this.config.defaultTTL){if(!this.isCacheEnabled())return;this.cache.size>=this.config.maxEntries&&this.removeOldestEntry();const s=Date.now();this.cache.set(e,{data:t,timestamp:s,expiresAt:s+o}),this.logDebug(`Cached data for key: ${e} (expires in ${o/1e3}s)`)}clear(e){e?(this.cache.delete(e),this.logDebug(`Cleared cache for key: ${e}`)):(this.cache.clear(),this.logDebug("Cleared entire cache"))}has(e){if(!this.isCacheEnabled())return!1;const t=this.cache.get(e);return t?t.expiresAt>Date.now():!1}getStats(){return{size:this.cache.size,keys:Array.from(this.cache.keys())}}cleanup(){const e=Date.now();let t=0;for(const[o,s]of this.cache.entries())s.expiresAt<=e&&(this.cache.delete(o),t++);t>0&&this.logDebug(`Removed ${t} expired entries from cache`)}isCacheEnabled(){return!0}removeOldestEntry(){let e=null,t=1/0;for(const[o,s]of this.cache.entries())s.timestamp<t&&(e=o,t=s.timestamp);e&&(this.cache.delete(e),this.logDebug(`Removed oldest entry from cache: ${e}`))}logDebug(e,t){this.config.debug&&(t?console.log(`[ApiCache] ${e}`,t):console.log(`[ApiCache] ${e}`))}}const K=new J({debug:!1,enableInDev:!0});function x(r,...e){const t=e.map(o=>typeof o=="object"?JSON.stringify(o):String(o)).join(":");return`${r}:${t}`}const z="https://api.groq.com/openai/v1/chat/completions",G="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",A={primaryId:"llama-3.3-70b-versatile",fallbackId:"llama3-70b-8192"},I={RESUME_ANALYSIS:{ttl:24*60*60*1e3},ATS_SCORE:{ttl:24*60*60*1e3},SKILLS_ANALYSIS:{ttl:24*60*60*1e3}},k=3,l={MAX_TOKENS_PER_REQUEST:6e3,MAX_RESPONSE_TOKENS:2e3,SYSTEM_MESSAGE_TOKENS:100,CONTEXT_TOKENS:200,MAX_REQUEST_SIZE:32768},m={TOKENS_PER_MINUTE:12e3,REQUESTS_PER_MINUTE:50,MIN_BACKOFF:2e3,MAX_BACKOFF:3e5,JITTER_MAX:2e3,COOLDOWN_PERIOD:6e4},i={tokens:0,requests:0,resetTime:Date.now(),lastRequestTime:0,consecutiveFailures:0},w=r=>new Promise(e=>setTimeout(e,r)),$=async r=>{const e=Math.random()*m.JITTER_MAX;await w(r+e)},M=r=>Math.min(m.MAX_BACKOFF,m.MIN_BACKOFF*Math.pow(2,r))+Math.random()*m.JITTER_MAX,q=async r=>{const e=Date.now();e-i.resetTime>=m.COOLDOWN_PERIOD&&(i.tokens=0,i.requests=0,i.resetTime=e,i.consecutiveFailures=0);const t=e-i.lastRequestTime,o=i.consecutiveFailures>0?M(i.consecutiveFailures):0;if(t<o){const s=o-t;console.log(`Applying backoff delay of ${s}ms (failures: ${i.consecutiveFailures})`),await $(s)}if(i.tokens+r>m.TOKENS_PER_MINUTE||i.requests+1>m.REQUESTS_PER_MINUTE){const s=m.COOLDOWN_PERIOD-(e-i.resetTime);console.log(`Rate limit approaching, waiting ${s}ms`),await $(s),i.tokens=r,i.requests=1,i.resetTime=Date.now()}else i.tokens+=r,i.requests+=1;i.lastRequestTime=Date.now()},g=r=>{if(!r)return 0;const e=r.trim().split(/\s+/).length,t=Math.ceil(e*.75);return Math.ceil(t*1.2)},T=r=>500+new TextEncoder().encode(r).length,v=(r,e)=>{if(!r)return r;let t=r;const o=g(t);if(o<=e)return t;console.log(`Content too large: ${o} tokens (max: ${e}), truncating...`);const s=t.split(/\n\s*\n/);for(;s.length>1&&g(s.join(`

`))>e;)s.pop();if(t=s.join(`

`),g(t)>e){const n=t.split(/[.!?]+\s+/);for(;n.length>1&&g(n.join(". "))>e;)n.pop();t=n.join(". ")+"."}for(;g(t)>e&&t.length>100;)t=t.substring(0,Math.floor(t.length*.9));const a=g(t);return console.log(`Truncated from ${o} to ${a} tokens`),t},b=(r,e)=>{if(!r)return r;let t=r;for(;T(t)>e&&t.length>100;){const o=t.split(/[.!?]+/);o.length>1?(o.pop(),t=o.join(".")+"."):t=t.substring(0,Math.floor(t.length*.8))}return t},U=(r,e)=>{if(!r)return[];const t=[],o=r.split(/\s+/);let s=[],a=0;for(const n of o){const c=g(n);a+c>e?(t.push(s.join(" ")),s=[n],a=c):(s.push(n),a+=c)}return s.length>0&&t.push(s.join(" ")),t},O=r=>{var e;try{return JSON.parse(r),r}catch{try{const t=r.match(/\{[\s\S]*\}/);if(t){const o=t[0];return JSON.parse(o),o}if(r.match(/\d+/)){const o=parseInt(((e=r.match(/\d+/))==null?void 0:e[0])||"65");return JSON.stringify({score:isNaN(o)?65:o})}}catch(t){console.error("Failed to extract JSON from response:",t)}return JSON.stringify({error:"Invalid JSON response",originalResponse:r})}},_=async(r,e,t)=>{const o=x("groq",`${e}:${r}`);return K.get(o,async()=>{var u,S,y;let s=null;const a=P.validateRequestSize(r);if(!a.isValid)throw new Error(a.message||"Request too large");const n=b(r,l.MAX_REQUEST_SIZE);n!==r&&console.log(`Truncated chunk from ${r.length} to ${n.length} characters to prevent 431 error`);const c=g(n);P.logRequest(a.size,c,"Processing chunk");for(let f=1;f<=k;f++)try{if(console.log(`Groq API attempt ${f}/${k} using model ${e}`),f>1){const p=M(i.consecutiveFailures);await w(p)}const h=JSON.stringify({model:e,messages:[{role:"system",content:"You are an AI assistant specialized in resume analysis and ATS optimization. You must respond in JSON format."},{role:"user",content:n+`

Respond with valid JSON format.`}],temperature:.1,max_tokens:l.MAX_RESPONSE_TOKENS,response_format:{type:"json_object"}}),E=T(h);if(E>l.MAX_REQUEST_SIZE)throw new Error(`Request body too large: ${E} bytes. Consider using shorter content.`);const d=await fetch(z,{method:"POST",headers:{"Content-Type":"application/json",Authorization:`Bearer ${G}`,"User-Agent":"ResumeAnalyzer/1.0",Accept:"application/json","Cache-Control":"no-cache"},body:h});if(d.status===429){const p=parseInt(d.headers.get("retry-after")||"60",10);i.consecutiveFailures++;const D=await d.text();throw new Error(`429 Too Many Requests: ${D}. Retry after ${p}s`)}if(d.status===431){const p=await d.text();throw console.error(`[431 Error] Request size: ${E} bytes, Chunk size: ${n.length} chars`),new Error(`431 Request Header Fields Too Large: ${p}. Request size: ${E} bytes`)}if(!d.ok){const p=await d.text();throw new Error(`Groq API returned ${d.status}: ${p}`)}const N=await d.json();if(!((y=(S=(u=N.choices)==null?void 0:u[0])==null?void 0:S.message)!=null&&y.content))throw new Error("Invalid response format from Groq API");return i.consecutiveFailures=0,N.choices[0].message.content}catch(h){if(s=h,console.error(`Groq API attempt ${f} failed:`,h),h instanceof Error&&h.message.includes("429")){const E=M(i.consecutiveFailures);console.log(`Rate limit hit, backing off for ${E/1e3} seconds`),await w(E);continue}if(h instanceof Error&&h.message.includes("431"))throw console.error("Request headers too large, content may need to be reduced further"),i.consecutiveFailures++,s;if(f<k){await w(m.MIN_BACKOFF*f);continue}throw s}throw s||new Error("All Groq API attempts failed")},t)},Q=r=>{try{const e=r.map(t=>{try{return JSON.parse(O(t))}catch(o){return console.error("Failed to parse chunk result:",o),{}}});if(e[0].score!==void 0){const t=e.reduce((o,s)=>o+(s.score||0),0);return JSON.stringify({score:Math.round(t/e.length)})}else if(e[0].matchedSkills!==void 0){const t={score:Math.round(e.reduce((o,s)=>o+(s.score||0),0)/e.length),matchedSkills:Array.from(new Set(e.flatMap(o=>o.matchedSkills||[]))),missingSkills:Array.from(new Set(e.flatMap(o=>o.missingSkills||[]))),feedback:e.map(o=>o.feedback||"").filter(Boolean).join(" ")};return JSON.stringify(t)}else{const t={atsScore:Math.round(e.reduce((o,s)=>o+(s.atsScore||0),0)/e.length),formatAnalysis:e[0].formatAnalysis||{score:70,feedback:"Format analysis not available"},contentAnalysis:{score:Math.round(e.reduce((o,s)=>{var a;return o+(((a=s.contentAnalysis)==null?void 0:a.score)||0)},0)/e.length),feedback:e.map(o=>{var s;return((s=o.contentAnalysis)==null?void 0:s.feedback)||""}).filter(Boolean).join(" ")},suggestions:e.flatMap(o=>o.suggestions||[])};return JSON.stringify(t)}}catch(e){throw console.error("Error merging results:",e),new Error("Failed to merge chunked results")}},L=async(r,e=I.RESUME_ANALYSIS)=>{let t=r;const o=T(t);o>l.MAX_REQUEST_SIZE&&(console.log(`Prompt too large (${o} bytes), truncating to prevent 431 error`),t=b(t,Math.floor(l.MAX_REQUEST_SIZE*.8)));const s=l.MAX_TOKENS_PER_REQUEST-l.MAX_RESPONSE_TOKENS-l.SYSTEM_MESSAGE_TOKENS-l.CONTEXT_TOKENS,a=g(t);if(a>s){console.log(`Splitting large prompt (${a} tokens) into chunks`);const n=U(t,s),c=[];for(const u of n){const S=T(u);if(S>l.MAX_REQUEST_SIZE){const f=b(u,Math.floor(l.MAX_REQUEST_SIZE*.8));console.log(`Chunk too large, truncated from ${S} to ${T(f)} bytes`)}const y=g(u);await q(y+l.SYSTEM_MESSAGE_TOKENS);try{const f=await _(u,A.primaryId,e);c.push(f)}catch(f){console.error("Error processing chunk:",f);try{const h=await _(u,A.fallbackId,e);c.push(h)}catch(h){if(console.error("Fallback model failed:",h),h instanceof Error&&h.message.includes("431"))throw new Error("Content too large for API processing. Please try with shorter content.")}}}if(c.length===0)throw new Error("All chunks failed to process");return Q(c)}await q(a+l.SYSTEM_MESSAGE_TOKENS);try{return await _(t,A.primaryId,e)}catch(n){return console.error("Primary model failed:",n),_(t,A.fallbackId,e)}},te=async r=>{var e;try{let o=r;r.length>12e3&&(console.log(`Resume text too long (${r.length} chars), truncating to 12000 chars for ATS scoring`),o=v(r,Math.floor(12e3/4)));const s=`Calculate the ATS compatibility score for this resume:

${o}

Provide only a number between 0-100 representing the ATS score. Return your answer in JSON format with a 'score' field, like {"score": 85}.`,a=await L(s,I.ATS_SCORE),n=O(a);try{const c=JSON.parse(n);return{score:typeof c.score=="number"?c.score:65}}catch(c){console.error("Failed to parse ATS score result:",c);const u=parseInt(((e=a.match(/\d+/))==null?void 0:e[0])||"65");return{score:isNaN(u)?65:Math.min(100,Math.max(0,u))}}}catch(t){return console.error("Error in Groq ATS scoring:",t),t instanceof Error&&t.message.includes("431")&&console.error("Resume content too large for ATS scoring."),{score:65}}},re=async(r,e)=>{try{let o=r;r.length>12e3&&(console.log(`Resume text too long (${r.length} chars), truncating to 12000 chars for skills analysis`),o=v(r,Math.floor(12e3/4)));const s=`Analyze the skills in this resume for the job role ${e}:

${o}

Provide a JSON response with fields: score (number), matchedSkills (array of strings), missingSkills (array of strings), and feedback (string with recommendations). Your response must be valid JSON format.`,a=await L(s,I.SKILLS_ANALYSIS),n=O(a);try{return JSON.parse(n)}catch(c){return console.error("Failed to parse skills analysis result:",c),C(e).skillsGap}}catch(t){return console.error("Error in Groq skills analysis:",t),t instanceof Error&&t.message.includes("431")&&console.error("Resume content too large for skills analysis."),C(e).skillsGap}},P={logRequest:(r,e,t)=>{console.log(`[Groq API] ${t} - Size: ${r} bytes, Tokens: ${e}`),r>l.MAX_REQUEST_SIZE*.8&&console.warn(`[Groq API] Large request detected: ${r} bytes (warning threshold: ${l.MAX_REQUEST_SIZE*.8})`)},validateRequestSize:r=>{const e=T(r);return e>l.MAX_REQUEST_SIZE?{isValid:!1,size:e,message:`Request too large: ${e} bytes exceeds limit of ${l.MAX_REQUEST_SIZE} bytes`}:{isValid:!0,size:e}}};export{re as analyzeSkillsWithGroq,te as getAtsScoreWithGroq};
